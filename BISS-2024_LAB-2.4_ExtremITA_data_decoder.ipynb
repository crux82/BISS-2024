{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crux82/BISS-2024/blob/main/BISS-2024_LAB-2.4_ExtremITA_data_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BISS-2024 Tutorial\n",
        "\n",
        "## LAB 2.4: Large Language Models and How to Instruction Tune Them (in a Sustainable Way)\n",
        "\n",
        "**Authors**: C.D. Hromei & D. Croce\n",
        "\n",
        "This is an implementation for training and using a Large Language Model (based on [LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)) with instructions in order to solve the linguistic tasks of [EVALITA](https://www.evalita.it/campaigns/evalita-2023/). In this lab, we will see how to encode datasets from any format to a sequence to sequence format, train the model using [Q-LoRA](https://arxiv.org/abs/2305.14314), perform the inference using the previous trained model for generating answers to instructions, and finally, how to encode back the data to the original format.. all of it using the only available *T4 GPU with 15GB from Google Colab*."
      ],
      "metadata": {
        "id": "uktLw9Hepf-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tutorial is split into 4 steps, reflecting the aforementioned process:\n",
        "- Step 1 - Encoding the data\n",
        "- Step 2 - Training the LLaMA model\n",
        "- Step 3 - Inference: generating answers\n",
        "- **Step 4 - Deconding the data**"
      ],
      "metadata": {
        "id": "pJ-gHwDRo9i1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Index:\n",
        "1. Introduction, Workflow and Objectives\n",
        "2. Preliminary steps\n",
        "3. Loading the data from previous step\n",
        "4. Decoding: generating the PubTator format\n",
        "5. Bonus: ExtremITA demo"
      ],
      "metadata": {
        "id": "j0aYBoWpV-qM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 - Decoding the data\n",
        "\n",
        "In this Notebook we will see the decoding part of the data, once the model generated its answer, in order to transform it back into the task specific format. Here we will focus on the [CLinkaRT](https://e3c.fbk.eu/clinkart) task: we will take the sequence of events (the tests) and results, we will look for the indices in the original text and we will produce a PubTator format-like file."
      ],
      "metadata": {
        "id": "2d2rcBXNorPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input\n",
        "The \"input\" of the Notebook is the previously generated 4-column file with the predictions for the CLinkaRT task."
      ],
      "metadata": {
        "id": "OM6GRZMmWQs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's decode"
      ],
      "metadata": {
        "id": "4Rqj8ZW1iMwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "yk41eme2Faio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOjYHOVYBYCy"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from os.path import isdir\n",
        "from os import mkdir\n",
        "import spacy\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA9uMtE6QfLh"
      },
      "source": [
        "### Handle external data:\n",
        "Download one example for the Clinkart dataset as labeled from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfUKY2XQRSaL",
        "outputId": "b977d08b-4cda-49de-c9d7-7f0ba56cd61f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-06 16:24:14--  https://raw.githubusercontent.com/crux82/CLiC-it_2023_tutorial/main/data/clinkart_predictions.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 153 [text/plain]\n",
            "Saving to: ‘clinkart_predictions.tsv’\n",
            "\n",
            "\rclinkart_prediction   0%[                    ]       0  --.-KB/s               \rclinkart_prediction 100%[===================>]     153  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-06 16:24:14 (13.1 MB/s) - ‘clinkart_predictions.tsv’ saved [153/153]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget -nc https://raw.githubusercontent.com/crux82/CLiC-it_2023_tutorial/main/data/clinkart_predictions.tsv\n",
        "\n",
        "input_file_path = \"clinkart_predictions.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cat clinkart_predictions.tsv"
      ],
      "metadata": {
        "id": "sS_I3qY2iwO7",
        "outputId": "42def7ac-51bd-4bd2-9c7b-0801af7dd3c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11_0_66\tclinkart\tVeniva documentato, inoltre, il rialzo della troponina TnT-hs (289; \t[BREL] 289 [SEP] troponina [EREL] [BREL] 289 [SEP] troponina [EREL]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------\n",
        "### Decode\n",
        "Now we will load the created file with the predictions for the CLinkaRT task and we will convert them back in the PubTator format. We will use the specific decoder from the list of [decoders](https://github.com/crux82/ExtremITA/tree/main/tasks).\n",
        "\n",
        "**Note**: this is the most complicated decoder among the EVALITA 2023 tasks. We invite you to take a look at the others."
      ],
      "metadata": {
        "id": "QextIpcZYIVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(preds):\n",
        "  out = dict()\n",
        "  texts = dict()\n",
        "\n",
        "  for example_pred in preds:\n",
        "    relations = []\n",
        "\n",
        "    id = example_pred[0]\n",
        "    text = example_pred[2]\n",
        "    prediction = example_pred[3]\n",
        "\n",
        "    doc_id, char_from, char_to = id.split(\"_\")\n",
        "    if not doc_id in out:\n",
        "        out[doc_id] = []\n",
        "    if not doc_id in texts:\n",
        "        texts[doc_id] = \"\"\n",
        "    texts[doc_id] += text\n",
        "\n",
        "    char_from = eval(char_from)\n",
        "    char_to = eval(char_to)\n",
        "    regex = re.compile(r\"\\[BREL\\].*?\\[SEP\\].*?\\[EREL\\]\")\n",
        "\n",
        "    matched_list = re.findall(regex, prediction)\n",
        "    for matched in matched_list:\n",
        "        valid = True\n",
        "        try:\n",
        "            tmp = re.sub(\"^\\[BREL\\] \", \"\", matched)\n",
        "            tmp = re.sub(\" \\[EREL\\]$\", \"\", tmp)\n",
        "            brel, erel = tmp.split(\" [SEP] \")\n",
        "        except:\n",
        "            valid = False\n",
        "\n",
        "        if \"[BREL]\" in brel or \"[SEP]\" in brel or \"[EREL]\" in brel or \"[BREL]\" in erel or \"[SEP]\" in erel or \"[EREL]\" in erel:\n",
        "            valid = False\n",
        "        if valid:\n",
        "            relations.append((brel, erel))\n",
        "\n",
        "    for brel, erel in relations:\n",
        "        try:\n",
        "            m_from_brel, m_to_brel = re.search(r\"\\b{}\\b\".format(brel), text).span()\n",
        "\n",
        "            # find the EREL closest to the BREL\n",
        "            min_dist = 10000000\n",
        "            for m in re.finditer(r\"\\b{}\\b\".format(erel), text):\n",
        "                f, _ = m.span()\n",
        "                if abs(f-m_from_brel)<min_dist:\n",
        "                    min_dist = abs(f-m_from_brel)\n",
        "                    m_from_erel, m_to_erel = m.span()\n",
        "        except:\n",
        "            continue # it can happen when the model hallucinates\n",
        "\n",
        "        # correct the offset wrt the sentence\n",
        "        m_from_brel += char_from\n",
        "        m_from_erel += char_from\n",
        "        m_to_brel += char_from\n",
        "        m_to_erel += char_from\n",
        "\n",
        "        obj = (brel, erel, m_from_brel, m_to_brel, m_from_erel, m_to_erel)\n",
        "        if obj not in out[doc_id]:\n",
        "            out[doc_id].append(obj)\n",
        "\n",
        "  with open(f\"clinkart.txt\", \"w\") as fo:\n",
        "      for doc_id, sentences in out.items():\n",
        "          text = texts[doc_id]\n",
        "          fo.write(f\"{doc_id}|t|{text}\\n\")\n",
        "          for brel, erel, m_from_brel, m_to_brel, m_from_erel, m_to_erel in sentences:\n",
        "              fo.write(f\"{doc_id}\\tREL\\t{m_from_brel}-{m_to_brel}\\t{m_from_erel}-{m_to_erel}\\t{brel}\\t{erel}\\n\")\n",
        "          fo.write(f\"\\n\")"
      ],
      "metadata": {
        "id": "qUn8wq5FIjPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load the previous generated file, we transform it into a list of lists of 4 elements and then produce the requested format."
      ],
      "metadata": {
        "id": "WPAfD3-NWlY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "predictions = []\n",
        "\n",
        "with open(input_file_path, 'r') as file:\n",
        "    reader = csv.reader(file, delimiter='\\t')\n",
        "\n",
        "    for row in reader:\n",
        "      predictions.append(row)\n",
        "\n",
        "decode(predictions)"
      ],
      "metadata": {
        "id": "tvGTu3uFcU_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we load the saved file, we can see what's inside."
      ],
      "metadata": {
        "id": "4Q0Y0_MNWvTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"clinkart.txt\", \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiR7jOCteZjI",
        "outputId": "a0972e3d-9b0a-4178-a852-1e4151b2251c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11|t|Veniva documentato, inoltre, il rialzo della troponina TnT-hs (289; \n",
            "\n",
            "11\tREL\t63-66\t45-54\t289\ttroponina\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding all the datasets\n",
        "We saw how to decode one dataset (CLinkaRT), but if you want to decode all the others [here](https://github.com/crux82/ExtremITA/tree/main/tasks) you can find the list with the links to the code. Each specific decoder will produce the task specific format file, ready to be submitted to the competition.\n",
        "\n",
        "If you clone the Github Repository, you can exploit the `decode` method in the root to automatically decode each dataset.\n",
        "\n",
        "- [acti](https://github.com/crux82/ExtremITA/blob/main/tasks/acti.py)\n",
        "- [clinkart](https://github.com/crux82/ExtremITA/blob/main/tasks/clinkart.py)\n",
        "- [discotex](https://github.com/crux82/ExtremITA/blob/main/tasks/discotex.py)\n",
        "- [emit](https://github.com/crux82/ExtremITA/blob/main/tasks/emit.py)\n",
        "- [emotivita](https://github.com/crux82/ExtremITA/blob/main/tasks/emotivita.py)\n",
        "- [geolingit](https://github.com/crux82/ExtremITA/blob/main/tasks/geolingit.py)\n",
        "- [haspeede](https://github.com/crux82/ExtremITA/blob/main/tasks/haspeede.py)\n",
        "- [hodi](https://github.com/crux82/ExtremITA/blob/main/tasks/hodi.py)\n",
        "- [langlearn](https://github.com/crux82/ExtremITA/blob/main/tasks/langlearn.py)\n",
        "- [multifakedetective](https://github.com/crux82/ExtremITA/blob/main/tasks/multifakedetective.py)\n",
        "- [nermud](https://github.com/crux82/ExtremITA/blob/main/tasks/nermud.py)\n",
        "- [politicit](https://github.com/crux82/ExtremITA/blob/main/tasks/politicit.py)\n",
        "- [wicita](https://github.com/crux82/ExtremITA/blob/main/tasks/wicita.py)"
      ],
      "metadata": {
        "id": "l6JowZbTQ5nl"
      }
    }
  ]
}